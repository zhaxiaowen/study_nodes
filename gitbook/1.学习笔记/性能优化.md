# 性能优化概念

# 什么是性能优化?

性能优化指再不影响系统运行正确性的前提下,使之运行得更快,完成特定功能所需的时间更短,或拥有更强大的服务能力.

# 关注

不同程序有不同的性能关注点,比如喝血计算关注运算速度,游戏引擎注重渲染效率,而后端服务追求吞吐能力.

服务器一般都是可水平扩展的分布式系统,系统处理能力取决于`单机负载能力和水平扩展能力`,所以,提升单机性能和提升水平扩展能力是两个主要方向,理论上系统水平方向可以无限扩展,但水平扩展后往往导致通行成本飙升(甚至瓶颈),同时面临单机处理能力下降的问题.

# 指标

衡量单机性能有很多指标,比如:QPS(Query Per Second)、TPS、OPS、IOPS、最大连接数、并发数等评估吞吐的指标.

CPU为了提高吞吐,会把指令执行分为多个阶段,会搞指令Pipeline,同样,软件系统为了提升处理能力,往往会引入批处理(攒包),跟CPU流水线会引起指令执行Latency增加一样,伴随着系统负载增加也会导致延迟(Latency)增加,可见,`系统吞吐和延迟是两个冲突的目标`.

显然,过高的延迟是不能接受的,所以,服务器性能优化的目标往往编程:`追求可容忍延迟(Latency)下的最大吞吐(Throughput)`.

延迟(也叫响应时间:RT)不是固定的,通常在一个范围内波动,我们可以用平均时延去评估系统性能,但有时候,平均时延是不够的,这很容易理解,比如80%的请求都在10ms以内得到响应,但20%的请求时延超过2s,而这20%的高延迟可能会引发投诉,同样不可接受.

一个改进措施是使用TP90、TP99之类的指标,它不是取平均,而是需确保排序后90%、99%请求满足时延超过2s,而这20%的高延迟可能会引发投诉,同样不可接受.

通常,执行效率(CPU)是我们的重点关注,但有时候,我们也需要关注内存占用、网络带宽、磁盘IO等,影响性能的因素很多,它是一个复杂而有趣的问题.

# 基础知识

能编写运行正确的程序不一定能做性能优化,性能优化有更高的要求,这样讲并不是想要吓阻想做性能优化的工程师,而是实事求是讲,性能优化既要扎实的系统知识,又需要丰富的实践经验,只有这样,你才能具备case by case分析问题解决问题的能力.

所以,相比直接给出结论,我更愿意花些篇幅讲一些基础知识,我坚持认为底层基础是理解并掌握性能优化技能的前提,值得花费一些时间研究并掌握这些根技术.

# CPU架构

需要了解CPU架构,理解运算单元、记忆单元、控制单元是如何既各司其职又相互配合完成工作的.

需要了解CPU如何读取数据,CPU如何执行任务.

需要了解数据总线,地址总线和控制总线的区别和作用.

需要了解指令周期:取指、译指、执行、写回.

需要了解CPU Pipeline,超标量流水线,乱序执行.

需要了解多CPU、多核心、逻辑核、超线程、多线程、协程这些概念.

# 存储金字塔

CPU的速度和访存速度相差200倍,高速缓存是跨越这个鸿沟的桥梁,你需要理解存储金字塔,而这个层次结构思维基于着一个称为`局部性原理`的思想,它对软硬件系统的设计和性能有着极大的影响.
局部性又分为时间局部和空间局部性.

# 缓存

现代计算机系统一般有L1-L2-L3三级缓存.

通过进入/sys/devices/system/cpu/cpu0/cache/index0 1 2 3目录下查看.

size对应大小、type对应类型、coherency_line_size对应cache line大小.

每个cpu核心有独立的L1、L2告诉缓存,所以L1和L2是on-chip缓存;L3是多个CPU核心共享的,它是off-chip缓存.

L1缓存又分为i-cache(指令缓存)和d0cache(数据缓存),L1缓存通常只有32k/64kb,速度高达4cycles.

L2缓存能到256kb,速度8 cycles左右.

L3则高达30MB,速度32 cycles左右.

而内存高达数G,访问时延则在200 cycles左右.

所以CPU->寄存器->L1->L2->L3->内存->磁盘构成存储层级结构:越靠近CPU,存储容量越小、速度越快、单位成本越高,越远离CPU,存储容量越大、速度越慢、单位成本越低.

# 虚拟存储器(VM)

# 汇编基础

读懂汇编很有必要,需要了解几种寻址模式,了解数据操作、分支、传送、控制跳转指令.

理解C语言的if else、while/do while/for、switch case、函数调用是怎么翻译成汇编代码.

理解ebp+esp寄存器在函数调用过程中是如何构建和撤销栈帧.

理解函数参数和返回值是怎么传递的.

# 异常和系统调用

异常会导致控制流突变,异常控制流发生在计算机系统的各个层次,异常可以分为四类:

**中断(interrupt)** :中断是异步发生的,来自处理器外部IO设备信号,中断处理程序分上下部.

**陷阱(trap)** :陷阱是有意的异常,是执行一条指令的结果,系统调用是通过陷阱实现的,陷阱在用户程序和内核之间提供一个像过程调用的接口:系统调用.

**故障(fault)** :故障由错误情况引起,它有可能被故障处理程序修复,故障发生,处理器讲控制转移到故障处理程序,缺页(Page Fault)是经典的故障实例.

**终止(abort)** :终止是不可恢复的致命错误导致的结果,通常是硬件错误,会终止程序的执行.

# 进程、线程、协程

进程跟线程的区别:线程是共享存储空间的,每个执行流有一个执行控制结构体,这里会有一个指针,指向地址空间结构,一个进程内的多个线程,通过指向同一地址结构实现共享同一虚拟地址空间.

通过fork创建子进程的时候,不会马上copy一份数据,而是推迟到子进程对地址空间进行改写,这样做是合理的,此即为COW(Copy On Write),在应用开发中,也有大量的泪水借鉴.

协程是用户态的多执行流,C语言提供makecontext/getcontext/swapcontext系列接口,很多协程库也是基于这些接口实现的,微信的协程库libco通过hook慢速系统调用(比如write、read)做到静默替换,非常巧妙.

# 怎么做性能优化

两个方向:提高运行速度+减少计算量.

性能优化监控先行,要基于数据而非基于猜测,要搭建能尽量模拟真实运行状态的压力测试环境,在此基于上获取的profiling数据才是有用的.

方法论:监控 -> 分析 -> 优化

性能优化一个重要原则就是用数据说话.

瓶颈点可能有很多,如果不解决狭窄的瓶颈点,性能优化就不能达到预期效果.所以性能优化之前一定要先进行性能测试,摸清家底,建立测试基线.

另外,性能优化要一个点一个点的做,做完一点,马上做性能验证.这样可以避免无用的修改.

# 几个具体问题

## 如何定位CPU瓶颈?

CPU是通常大家最先关注的性能指标,宏观维度有核的CPU使用率,微观有函数的CPU cycle数,根据性能的模型,性能规格于CPU使用率是互相关联的,规格越高,CPU使用率越高,但是处理器的性能往往又受到内存带宽、Cache、发热等因素的一个像,所以CPU使用率和规格参数之间并不是简单的现行关系,所以性能规格翻倍并不能简单翻译成我们的CPU使用率要优化一倍.

至于CPU瓶颈的定位工具,最有名的工具是perf,它是性能分析的第一步,可以帮助我们找到系统的热点函数.

所以我们通过性能分析工具PMU或者其他工具去进一步分析CPU热点的原因比如是指令数本事就比较多,还是Cache miss导致的等,这样在做性能优化的时候不会走偏.

## 如何定位IO瓶颈?

系统IO的瓶颈可以通过CPU和负载的非线性关系体现出来.当负载增大时,系统吞吐量不能有效增大,CPU不能线性增长,其中一种可能是IO出现阻塞.

系统的队列长度特别是发送、写磁盘线程的队列长度也是IO瓶颈的一个间接指标.

对于网络系统来讲,建议先从外部观察系统.所谓的从外部观察是指通过观察外部的网络报文交换,可以用tcpdump,wireshark等工具抓包看下.

比如优化一个RPC项目,它的吞吐量是10TPS,客户希望是100TPS,我们使用wireshark抓取TCP报文流,可以分析报文之间的时间戳,响应延迟等指标来判断是否是由网络引起的.

然后可以通过netstat -i/-s 选项查看网络错误、重传等统计信息.

还可以通过iostat查看cpu等待IO的比例.

IO的概念也可以扩展到进程间通信.

对于磁盘类的应用程序,我们最希望看到写磁盘有没有时延、频率如何.其中一个方法就是通过内核ftrace、perf-event时间来动态观测系统.比如记录写块设备的起始和返回时间,这儿样我们就可以知道磁盘写是否有延时,也可以统计写磁盘时间耗费分布.有一个开源的工具包perf-tools里面包含着iolatency,iosnoop等工具.

# 如何定位锁的问题?

大家都知道锁会引入额外开销,但锁的开销到底有多大,估计很多人没有实测过,我可以给一个数据,一般单次加解锁100 cycles,spinlock或者cas更快一点.

使用锁的时候,要注意锁的粒度,要注意锁的粒度,但锁的粒度也不是越小越好,太大会增加撞锁的概率,太小会导致代码更难写.

多线程场景下,如果cpu利用率上不去,而系统吞吐也上不去,那就有可能是锁导致的性能下降,这个时候,可以观察程序的sys cpu和usr cpu,这个时候通过perf如果发现lock的开销大,那就没错了.

如果程序卡住了,可以用pstack把堆栈打出来,定位思索的问题.

# 如何提高cache利用率?

内存/cache问题是常见的负载瓶颈问题,通常可利用perf等一些通用工具来辅助分析,优化cache的思想可以从两方面来着手,一个是增加局部数据/代码的连续性,提升cacheline的利用率,减少cache miss,另一个是通过prefetch,降低miss带来的开销.

通过对数据/代码根据领热进行重排分区,可提升cacheline的有效利用率,当然触发false-sharing另当别论,这个需要根据运行trace进行深入调整了.

说到prefetch,用过的人往往都有一种体会,现实效果比预期差的比较远,确实无论是数据prefetch还是代码prefetch,不确定性太大,我们和无线做过一些实践,最终以无线输出预期pattern,编译器自动插入prefetch的方案,效果还算可以.











































